{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79546911",
   "metadata": {},
   "source": [
    "# MLVC Exercise 3\n",
    "\n",
    "This notebook contains the third exercise of the Machine Learning for Visual Computing (193.189) lecture at TU Wien. Assignment via TUWEL. Please be aware of the deadlines in TUWEL.\n",
    "\n",
    "* Upload a zip-file with the required programms. The programming language is python. \n",
    "    1. Self-Attention --> `solutions/self_attention.py`\n",
    "    2. Vision Transformer --> `solutions/vision_transformer.py`\n",
    "    3. Masked Autoencoder --> `solutions/masked_autoencoder.py`\n",
    "* If you needed additional conda or pip packages add an anaconda environment.yml to the zip-file\n",
    "    1. conda env export > environment.yml\n",
    "    2. See --> https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#sharing-an-environment\n",
    "\n",
    "### Conda instructions\n",
    "\n",
    "1. conda create --name MLVC python=3.11\n",
    "2. conda activate MLVC\n",
    "3. Install the correct version of pytorch:\n",
    "    - CPU: python -m pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "    - GPU: python -m pip install torch torchvision --index-url https://download.pytorch.org/whl/cu129\n",
    "4. python -m pip install pillow matplotlib tqdm torchinfo pandas ipykernel nbformat ipywidgets scikit-learn tabulate seaborn umap-learn cvxopt\n",
    "5. Add python kernel to Jupyter Notebooks\n",
    "    - python -m ipykernel install --user --name MLVC --display-name \"Python (mlvc)”\n",
    "\n",
    "### Note 1: You may reuse the conda environments for all exercises!\n",
    "### Note 2: You may also use different package managers (e.g. pip, mamba, ...)!\n",
    "### Note 3: You do not need to exactly match the expected results: similar performance is sufficient.\n",
    "\n",
    "### **Please update your group number in the filename (_X) and only upload your solutions folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2132733",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from tabulate import tabulate\n",
    "import seaborn as sns\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import umap\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import pick_device, print_device_info\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# General setting for the ipynb\n",
    "NUM_SAMPLES = 10000  # Number of training samples that will be generated, do not exceed 25.000 on the TUWEL Jupyter Notebooks (will crash kernel)\n",
    "TEST_RATIO = 0.9  # Percentage of the generated samples that are used for training (rest is used for testing). [0, 1]\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "device = pick_device()\n",
    "print_device_info(device)\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "d_model = 32\n",
    "num_heads = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622c009",
   "metadata": {},
   "source": [
    "## Generate and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda5660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset using numpy and pillow\n",
    "\n",
    "from utils import Dataset, TorchDataset\n",
    "\n",
    "(dataset_train, labels_train), (dataset_test, labels_test) = Dataset(\n",
    "    NUM_SAMPLES, TEST_RATIO\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(2, 8, figsize=(8, 2), dpi=200)\n",
    "\n",
    "for i, ax in enumerate(axs.reshape(-1)):\n",
    "    ax.imshow(dataset_train[i, :].reshape((16, 16)), cmap=\"gray\")\n",
    "    ax.set_title(\"Circle\" if labels_train[i] == -1 else \"Square\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(dataset_train.shape, labels_train.shape, dataset_test.shape, labels_test.shape)\n",
    "\n",
    "# Convert to PyTorch datasets and dataloaders\n",
    "\n",
    "train_ds = TorchDataset(dataset_train, labels_train)\n",
    "test_ds = TorchDataset(dataset_test, labels_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "val_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea1356",
   "metadata": {},
   "source": [
    "## 1. Self-Attention\n",
    "\n",
    "Implement Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions import MultiHeadSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb68184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mhsa = MultiHeadSelfAttention(d_model=d_model, num_heads=num_heads).to(device)\n",
    "print(mhsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c1c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Make Q and K ~ alpha * I to emphasize self-similarity over cross-similarity\n",
    "    alpha = 5.0\n",
    "    mhsa.q_proj.weight.copy_(alpha * torch.eye(d_model))\n",
    "    mhsa.q_proj.bias.zero_()\n",
    "    mhsa.k_proj.weight.copy_(alpha * torch.eye(d_model))\n",
    "    mhsa.k_proj.bias.zero_()\n",
    "\n",
    "    # V and out as identity so outputs reflect the attention-weighted input directly\n",
    "    mhsa.v_proj.weight.copy_(torch.eye(d_model))\n",
    "    mhsa.v_proj.bias.zero_()\n",
    "    mhsa.out_proj.weight.copy_(torch.eye(d_model))\n",
    "    mhsa.out_proj.bias.zero_()\n",
    "\n",
    "# Sequence of S one-hot vectors in R^{d_model}\n",
    "x = torch.randn(1, 4*4, d_model, device=device)\n",
    "y = mhsa(x)  # (1, S, d_model)\n",
    "\n",
    "# Because attention is ~identity, each output token should be close to its input token\n",
    "max_abs_err = (y - x).abs().max().item()\n",
    "\n",
    "print(f\"Output shape: {tuple(y.shape)}\")\n",
    "print(f\"Max |y - x| = {max_abs_err:.6f}  (smaller is better; increase alpha to make it smaller)\")\n",
    "\n",
    "attn = mhsa.last_attn[0, 0].detach().cpu()  # (S, S) head 0\n",
    "plt.figure()\n",
    "plt.imshow(attn, aspect=\"auto\")\n",
    "plt.title(\"Demo 2: Attention heatmap (Head 0) — near identity\")\n",
    "plt.xlabel(\"Keys (positions)\"); plt.ylabel(\"Queries (positions)\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75eca1",
   "metadata": {},
   "source": [
    "#### Multi Head Attention Expected Results\n",
    "\n",
    "<div style=\"display:flex; justify-content:center; background-color:white;\">\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/identity_1.png\" style=\"height:100%; width:auto;\">\n",
    "  </div>\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/identity_2.png\" style=\"height:100%; width:auto;\">\n",
    "  </div>\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/identity_3.png\" style=\"height:100%; width:auto;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4fe68d",
   "metadata": {},
   "source": [
    "## 2. Transformer and Tokens\n",
    "\n",
    "Patchify, Positional Encodings and Class Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd253a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions import ViTClassifier\n",
    "from utils import plot_results_attention, plot_attn_per_head_for_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea0e96c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34951a4",
   "metadata": {},
   "source": [
    "### Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import fit_classifier\n",
    "\n",
    "# Example how to reshape a single image from the dataset (1D array of length 256) to a 2D image (16x16)\n",
    "first_val_image = torch.tensor(dataset_test[0].reshape(16, 16), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad64c375",
   "metadata": {},
   "source": [
    "## Baseline Transformer\n",
    "\n",
    "- Every token embedding is independent, but the model has no way to know the order or layout.\n",
    "- Attention is permutation-invariant, so without positional signal, it can only average across pixels.\n",
    "\n",
    "    → Model is unable to learn useful structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e9f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTClassifier(d_model=d_model, num_heads=num_heads, use_cls_token=False, pos_emb=None, patchify=False)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {total_params} trainable parameters.\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history_baseline, model = fit_classifier(model, train_loader, val_loader, optimizer, criterion, device=device, epochs=epochs)\n",
    "plot_attn_per_head_for_query(model, first_val_image, device=device, block_idx=0, query_rc=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a75767",
   "metadata": {},
   "source": [
    "#### Baseline Expected Results\n",
    "\n",
    "<div style=\"display:flex; justify-content:center; background-color:white;\">\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/attn_per_head_1.png\" style=\"height:100%; width:auto;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035a514",
   "metadata": {},
   "source": [
    "## Transformer + Positional embeddings\n",
    "\n",
    "- Now the model knows where each token comes from.\n",
    "- Even if embeddings are trivial, attention can exploit relative positions.\n",
    "\n",
    "    → The first point where transformers start to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTClassifier(d_model=d_model, num_heads=num_heads, use_cls_token=False, pos_emb=\"sinusoidal\", patchify=False)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {total_params} trainable parameters.\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history_posemb, model = fit_classifier(model, train_loader, val_loader, optimizer, criterion, device=device, epochs=epochs)\n",
    "plot_attn_per_head_for_query(model, first_val_image, device=device, block_idx=0, query_rc=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddd2096",
   "metadata": {},
   "source": [
    "#### Positional Embeddings Expected Results\n",
    "\n",
    "<div style=\"display:flex; justify-content:center; background-color:white;\">\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/attn_per_head_2.png\" style=\"height:100%; width:auto;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3a1767",
   "metadata": {},
   "source": [
    "## Transformer + Positional embeddings + CLS token\n",
    "\n",
    "- Adds a dedicated representation for classification.\n",
    "- Without it, one relies on mean-pooling, which sometimes washes out details.\n",
    "\n",
    "    → Gives the model a single query that “asks” the sequence for class information.\n",
    "\n",
    "Note: For small and shallow transformer architectures the class token does not improve accuracy, and instead actually reduces it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTClassifier(d_model=d_model, num_heads=num_heads, use_cls_token=True, pos_emb=\"sinusoidal\", patchify=False)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {total_params} trainable parameters.\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history_cls, model = fit_classifier(model, train_loader, val_loader, optimizer, criterion, device=device, epochs=epochs)\n",
    "plot_attn_per_head_for_query(model, first_val_image, device=device, block_idx=0, query_rc=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc001f1",
   "metadata": {},
   "source": [
    "#### Class Token Expected Results\n",
    "\n",
    "<div style=\"display:flex; justify-content:center; background-color:white;\">\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/attn_per_head_3.png\" style=\"height:100%; width:auto;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec61011",
   "metadata": {},
   "source": [
    "## Transformer + Positional embeddings + CLS token + Patchify\n",
    "\n",
    "- Instead of 256 near-duplicate pixel tokens, the image is compressed into fewer, richer tokens with learnable embeddings.\n",
    "- Reduces sequence length (faster training, less overfitting) and injects spatial inductive bias.\n",
    "\n",
    "    → Huge performance jump, because the input representation is informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd965ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTClassifier(d_model=d_model, num_heads=num_heads, use_cls_token=True, pos_emb=\"sinusoidal\", patchify=True)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {total_params} trainable parameters.\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history_patchify, model = fit_classifier(model, train_loader, val_loader, optimizer, criterion, device=device, epochs=epochs)\n",
    "plot_attn_per_head_for_query(model, first_val_image, device=device, block_idx=0, query_rc=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e089969f",
   "metadata": {},
   "source": [
    "#### Patchify Expected Results\n",
    "\n",
    "<div style=\"display:flex; justify-content:center; background-color:white;\">\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/attn_per_head_4.png\" style=\"height:100%; width:auto;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9286e834",
   "metadata": {},
   "source": [
    "## Transformer + Learnable positional embeddings + CLS token + Patchify\n",
    "\n",
    "- Each patch gets its own dense vector, not just repeated scalars.\n",
    "- This is the full ViT architecture: patch projection + CLS token + positional embeddings.\n",
    "\n",
    "    → Best balance of learnability, efficiency, and expressiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTClassifier(d_model=d_model, num_heads=4, use_cls_token=True, pos_emb=\"learnable\", patchify=True)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {total_params} trainable parameters.\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history_learn_posemb, model = fit_classifier(model, train_loader, val_loader, optimizer, criterion, device=device, epochs=epochs)\n",
    "plot_attn_per_head_for_query(model, first_val_image, device=device, block_idx=0, query_rc=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca168d5b",
   "metadata": {},
   "source": [
    "#### Learnable Positional Embedding Expected Results\n",
    "\n",
    "<div style=\"display:flex; justify-content:center; background-color:white;\">\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/attn_per_head_5.png\" style=\"height:100%; width:auto;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e94ec44",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5214a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_histories = {\n",
    "    \"Baseline\": history_baseline,\n",
    "    \"PosEmb\": history_posemb,\n",
    "    \"ClsToken\": history_cls,\n",
    "    \"Patchify\": history_patchify,\n",
    "    \"LearnablePosEmb\": history_learn_posemb\n",
    "}\n",
    "plot_results_attention(all_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aa962a",
   "metadata": {},
   "source": [
    "#### ViT Trainings Expected Results\n",
    "\n",
    "<div style=\"display:flex; justify-content:center; background-color:white;\">\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/attn_per_head_plot.png\" style=\"height:100%; width:auto;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ccf84d",
   "metadata": {},
   "source": [
    "# 3. Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab8f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions import TinyMAE\n",
    "from utils import plot_example_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c78fb6",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import fit_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c096fe",
   "metadata": {},
   "source": [
    "## Self-Supervised Pretraining\n",
    "\n",
    "### MAE Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b05756",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_mae = 1e-3\n",
    "batch_size_mae = 64\n",
    "mask_ratio = 0.5\n",
    "patch_size = 4\n",
    "mae_epochs = 50\n",
    "\n",
    "train_loader_mae = DataLoader(train_ds, batch_size=batch_size_mae, shuffle=True, drop_last=False)\n",
    "val_loader_mae  = DataLoader(test_ds,  batch_size=batch_size_mae, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b33276",
   "metadata": {},
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d0e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = TinyMAE(d_model=d_model, patch_size=patch_size, masking=\"random\", mask_ratio=mask_ratio).to(device)\n",
    "optimizer = torch.optim.Adam(mae.parameters(), lr=learning_rate_mae)\n",
    "history_mae, mae = fit_mae(mae, train_loader_mae, val_loader_mae, optimizer, device=device, epochs=mae_epochs)\n",
    "plot_results_attention({\"MAE Pretraining\": history_mae})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126fa574",
   "metadata": {},
   "source": [
    "#### MAE Random Expected Results\n",
    "\n",
    "<div style=\"display:flex; justify-content:center; background-color:white;\">\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/mae_random.png\" style=\"height:100%; width:auto;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_length = len(dataset_test)\n",
    "random_indices = np.random.choice(dataset_length, size=3, replace=False)\n",
    "\n",
    "for i in random_indices:\n",
    "    val_image = torch.tensor(dataset_test[i].reshape(16, 16), dtype=torch.float32)\n",
    "    plot_example_mae(mae, val_image, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8d98ba",
   "metadata": {},
   "source": [
    "#### MAE Random Reconstruction Expected Results\n",
    "\n",
    "<div style=\"display:flex; flex-direction:column; align-items:center; background-color:white;\">\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/mae_reconst_random_1.png\" style=\"max-width:100%; height:auto;\">\n",
    "  </div>\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/mae_reconst_random_2.png\" style=\"max-width:100%; height:auto;\">\n",
    "  </div>\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/mae_reconst_random_3.png\" style=\"max-width:100%; height:auto;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad555658",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d953aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = TinyMAE(d_model=d_model, patch_size=patch_size, masking=\"block\", mask_ratio=mask_ratio).to(device)\n",
    "optimizer = torch.optim.Adam(mae.parameters(), lr=learning_rate_mae)\n",
    "history_mae, mae = fit_mae(mae, train_loader_mae, val_loader_mae, optimizer, device=device, epochs=mae_epochs)\n",
    "plot_results_attention({\"MAE Pretraining\": history_mae})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383a52df",
   "metadata": {},
   "source": [
    "#### MAE BLock Expected Results\n",
    "\n",
    "<div style=\"display:flex; justify-content:center; background-color:white;\">\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/mae_block.png\" style=\"height:100%; width:auto;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6be310",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_length = len(dataset_test)\n",
    "\n",
    "for i in random_indices:\n",
    "    val_image = torch.tensor(dataset_test[i].reshape(16, 16), dtype=torch.float32)\n",
    "    plot_example_mae(mae, val_image, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54434fb",
   "metadata": {},
   "source": [
    "#### MAE Block Reconstruction Expected Results\n",
    "\n",
    "<div style=\"display:flex; flex-direction:column; align-items:center; background-color:white;\">\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/mae_reconst_block_1.png\" style=\"max-width:100%; height:auto;\">\n",
    "  </div>\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/mae_reconst_block_2.png\" style=\"max-width:100%; height:auto;\">\n",
    "  </div>\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/mae_reconst_block_3.png\" style=\"max-width:100%; height:auto;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef13b339",
   "metadata": {},
   "source": [
    "## Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d44e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = TinyMAE(d_model=d_model, patch_size=patch_size, masking=\"grid\", mask_ratio=mask_ratio).to(device)\n",
    "optimizer = torch.optim.Adam(mae.parameters(), lr=learning_rate_mae)\n",
    "history_mae, mae = fit_mae(mae, train_loader_mae, val_loader_mae, optimizer, device=device, epochs=mae_epochs)\n",
    "plot_results_attention({\"MAE Pretraining\": history_mae})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1ae5f1",
   "metadata": {},
   "source": [
    "#### MAE Grid Expected Results\n",
    "\n",
    "<div style=\"display:flex; justify-content:center; background-color:white;\">\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/mae_grid.png\" style=\"height:100%; width:auto;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cbcdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_length = len(dataset_test)\n",
    "\n",
    "for i in random_indices:\n",
    "    val_image = torch.tensor(dataset_test[i].reshape(16, 16), dtype=torch.float32)\n",
    "    plot_example_mae(mae, val_image, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da45a2b1",
   "metadata": {},
   "source": [
    "#### MAE Grid Reconstruction Expected Results\n",
    "\n",
    "<div style=\"display:flex; flex-direction:column; align-items:center; background-color:white;\">\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/mae_reconst_grid_1.png\" style=\"max-width:100%; height:auto;\">\n",
    "  </div>\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/mae_reconst_grid_2.png\" style=\"max-width:100%; height:auto;\">\n",
    "  </div>\n",
    "  <div style=\"text-align:center; padding:10px;\">\n",
    "    <img src=\"assets/mae_reconst_grid_3.png\" style=\"max-width:100%; height:auto;\">\n",
    "  </div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLVC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
